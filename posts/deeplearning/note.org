#+BEGIN_COMMENT
.. title: note
.. slug: note
.. date: 2019-01-13 12:58:05 UTC+08:00
.. tags: 
.. category: deeplearning
.. link: 
.. description: 
.. type: text
#+END_COMMENT

深度学习汇总一些基本公式的记录
* 损失函数
** 均方误差
#+BEGIN_SRC python
  def mean_squared_error(y, t):
      return 0.5 * np.sum((y - t) * 2)
#+END_SRC

** 交叉熵误差
*** 监督数据是one-hot形式[0, 1, 0]
#+BEGIN_SRC python
  def cross_entropy_error(y, t):
      if y.ndim == 1:
          y = y.reshape(1, y.size)
          t = t.reshape(1, t.size)

      batch_size = y.shape[0]
      return -np.sum(t * np.log(y + delta)) / batch_size
#+END_SRC

*** 监督数据是标签形式（非one-hot表示，而是像"2", "7"这样的标签）
#+BEGIN_SRC python
  def cross_entropy_error(y, t):
      delta = 1e-7
      if y.ndim == 1:
          y = y.reshape(1, y.size)
          t = t.reshape(1, t.size)

      size = y.shape[0]
      return -np.sum(
          np.log(y[np.arange(size), t] + 1e-7)) / batch_size
#+END_SRC

* 梯度计算
** x为一维数组
#+BEGIN_SRC python
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)

    for idex in range(x.size):
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = fx(x)

        x[idx] = tmp_val - h
        fxh2 = fx(x)
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        x[idx] = tmp_val

    return grad
#+END_SRC

** x为二维数组
#+BEGIN_SRC python
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_like(x)

    height, width = grad.shape
    print(height)
    print(width)

    for i in range(height):
        for j in range(width):
            tmp_val = x[i, j]
            x[i, j] = tmp_val + h
            fxh1 = f(x)

            x[i, j] = tmp_val -h
            fxh2 = f(x)
            grad[i, j] = (fxh1 - fxh2) / (2 * h)
            x[i, j] = tmp_val
    return grad
#+END_SRC


* 激活函数
** softmax函数
#+BEGIN_SRC python
  import numpy as np

  def softmax(x):
      exp_a = np.exp(a)
      sum_exp_a = np.sum(exp_a)
      return exp_a / sum_exp_a
#+END_SRC
